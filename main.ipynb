{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Project One: TripAdvisor Recommendation Challenge - Beating BM25**\n",
    "_**Authors:** Alberto MARTINELLI, Alessia SARRITZU_\n",
    "\n",
    "### **Introduction**\n",
    "The goal of this project is to develop an unsupervised recommendation system that uses user reviews to suggest similar locations, outperforming the BM25 baseline. The system is evaluated using **Mean Squared Error (MSE)** between query and recommended location ratings, focusing exclusively on review text.\n",
    "\n",
    "### **Development Phases**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Filter reviews to include only those with ratings strictly covering seven aspects: **service**, **cleanliness**, **overall**, **value**, **location**, **sleep quality**, and **rooms**.\n",
    "   - Concatenate reviews by `offering_id` and compute average ratings for each aspect to represent each location.\n",
    "   - Take a random sample of 100 queries from the dataset for consistent evaluation of model performance.\n",
    "\n",
    "2. **Data Pre-Processing:**\n",
    "   - Apply text preprocessing to standardize review text:\n",
    "     - Tokenization: Split text into words.\n",
    "     - Stop word removal: Exclude common, irrelevant words.\n",
    "     - Lemmatization: Reduce words to their base forms.\n",
    "\n",
    "3. **BM25 Implementation:**\n",
    "   - Use the **Rank-BM25** library to recommend locations based on textual similarity.\n",
    "   - Evaluate performance by calculating MSE between query and recommended location ratings.\n",
    "\n",
    "4. **Enhanced Model Implementation:**\n",
    "   - Create a more advanced unsupervised model to outperform BM25.\n",
    "   - Use **TF-IDF vectorization** and **cosine similarity** to capture semantic relationships between reviews.\n",
    "\n",
    "5. **Evaluation and Comparison:**\n",
    "   - Compute MSE for both BM25 and the enhanced model across the test set.\n",
    "   - Compare results to determine the improved performance of the enhanced model.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. **Data and Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import swifter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "---\n",
    "1. **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_preparation(df):\n",
    "    # --------------------------- Filter data --------------------------------------------\n",
    "    required_aspects = {\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"}\n",
    "    filtered_df = df[df['ratings'].apply(lambda x: set(eval(x).keys()) == required_aspects)]\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]    2.1 Data filtered\")\n",
    "\n",
    "    # -------------------------- Take a sample for model testing --------------------------\n",
    "    sample_df = filtered_df.sample(n=100, random_state=42)\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]    2.2 Sample of 100 queries retrieved for model testing\")\n",
    "\n",
    "    # ------------------ Concatenate reviews for the same place ---------------------------\n",
    "    filtered_df.loc[:, 'ratings'] = filtered_df['ratings'].apply(eval)\n",
    "    expanded_ratings_df = pd.json_normalize(filtered_df['ratings']).join(filtered_df[['offering_id', 'title', 'text']])\n",
    "\n",
    "    # Calculate the mean of each rating aspect and concatenate reviews\n",
    "    final_df = expanded_ratings_df.groupby('offering_id').agg(\n",
    "        service=('service', 'mean'),  \n",
    "        cleanliness=('cleanliness', 'mean'),\n",
    "        overall=('overall', 'mean'),\n",
    "        value=('value', 'mean'),\n",
    "        location=('location', 'mean'),\n",
    "        sleep_quality=('sleep_quality', 'mean'),\n",
    "        rooms=('rooms', 'mean'),\n",
    "        text=('text', lambda x: ' '.join(x)), \n",
    "    ).reset_index()\n",
    "\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]    2.3 Reviews concatenated\")\n",
    "    return sample_df, final_df\n",
    "\n",
    "sample_df, final_df = dataframe_preparation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. **Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_file = \"final_df_preprocessed.csv\"\n",
    "if os.path.exists(preprocessed_file):\n",
    "    final_df = pd.read_csv(preprocessed_file)\n",
    "else:\n",
    "    final_df['text'] = final_df['text'].swifter.apply(preprocess_text)\n",
    "    final_df.to_csv(preprocessed_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. **BM25 Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query details extraction \n",
    "def extract_query(query_row, aspects, df):\n",
    "    query_id = query_row['offering_id']\n",
    "    query_text = query_row['text']\n",
    "    place_ratings = df[df['offering_id'] == query_id][aspects].iloc[0]\n",
    "    return query_id, query_text, place_ratings\n",
    "\n",
    "# BM25 implementation\n",
    "def apply_bm25(query_id, query_text, place_ratings, df, aspects):\n",
    "    # Exclude the query place from the documents to avoid recommending it\n",
    "    documents_df = df[df['offering_id'] != query_id].reset_index(drop=True)\n",
    "\n",
    "    # Tokenize each document for BM25\n",
    "    documents_df['text'] = documents_df['text'].astype(str)\n",
    "    documents = documents_df['text'].apply(lambda x: x.split())\n",
    "    bm25 = BM25Okapi(documents)\n",
    "    scores = bm25.get_scores(query_text.split())\n",
    "    top_match_index = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[0]\n",
    "    top_match = documents_df.iloc[top_match_index]\n",
    "\n",
    "    # Calculate the MSE between the ratings of the query place and the recommended place\n",
    "    recommended_ratings = top_match[aspects]\n",
    "    mse = mean_squared_error(place_ratings, recommended_ratings)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] 4. Processing data (BM25)\")\n",
    "output_file = \"results.csv\"\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\"row_id\", \"offering_id\", \"bm25_mse\"]).to_csv(output_file, index=False)\n",
    "\n",
    "existing_results = pd.read_csv(output_file)\n",
    "processed_ids = set(existing_results[~existing_results[\"bm25_mse\"].isna()][\"row_id\"].values)  # Exclude rows with missing bm25_mse\n",
    "\n",
    "for index, row in sample_df.iterrows():\n",
    "    # Process rows if they are not processed or have missing bm25_mse\n",
    "    if index not in processed_ids or pd.isna(existing_results.loc[existing_results[\"row_id\"] == index, \"bm25_mse\"].values[0]):\n",
    "        query_id, query_text, place_ratings = extract_query(row, aspects, final_df)\n",
    "        mse = apply_bm25(query_id, query_text, place_ratings, final_df, aspects)\n",
    "        \n",
    "        if index in existing_results[\"row_id\"].values:\n",
    "            existing_results.loc[existing_results[\"row_id\"] == index, \"bm25_mse\"] = mse\n",
    "        else:\n",
    "            new_row = pd.DataFrame([{\"row_id\": index, \"offering_id\": query_id, \"bm25_mse\": mse}])\n",
    "            existing_results = pd.concat([existing_results, new_row], ignore_index=True)\n",
    "        \n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]    -> Row {index}: processed with BM25 MSE={mse}\")\n",
    "\n",
    "existing_results.to_csv(output_file, index=False)\n",
    "if not existing_results.empty:\n",
    "    overall_average_mse = existing_results[\"bm25_mse\"].mean()\n",
    "    res = overall_average_mse\n",
    "else:\n",
    "    res = \"No data available in the file\"\n",
    "print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]    4.1 Calculating average MSE: {res}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4. **Enhanced Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
